{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3724153,"sourceType":"datasetVersion","datasetId":2151228},{"sourceId":11381888,"sourceType":"datasetVersion","datasetId":7126743}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch torchvision matplotlib\n!pip install -q wandb\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:21:27.184753Z","iopub.execute_input":"2025-04-13T09:21:27.185020Z","iopub.status.idle":"2025-04-13T09:22:54.740477Z","shell.execute_reply.started":"2025-04-13T09:21:27.184998Z","shell.execute_reply":"2025-04-13T09:22:54.739333Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:22:54.742262Z","iopub.execute_input":"2025-04-13T09:22:54.742623Z","iopub.status.idle":"2025-04-13T09:23:04.790469Z","shell.execute_reply.started":"2025-04-13T09:22:54.742590Z","shell.execute_reply":"2025-04-13T09:23:04.789593Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_path = \"/kaggle/input/person-face-sketches/train\"\nprint(\"Contents of /train:\", os.listdir(train_path))\n\ntest_path = \"/kaggle/input/person-face-sketches/test\"\nprint(\"Contents of /test:\", os.listdir(test_path))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:23:04.791400Z","iopub.execute_input":"2025-04-13T09:23:04.791695Z","iopub.status.idle":"2025-04-13T09:23:04.818807Z","shell.execute_reply.started":"2025-04-13T09:23:04.791667Z","shell.execute_reply":"2025-04-13T09:23:04.817961Z"}},"outputs":[{"name":"stdout","text":"Contents of /train: ['photos', 'sketches']\nContents of /test: ['photos', 'sketches']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class FaceSketchDataset(Dataset):\n    def __init__(self, base_path, folders=[\"train\", \"test\", \"val\"], transform=None):\n        self.photo_paths = []\n        self.sketch_paths = []\n        self.transform = transform\n\n        for folder in folders:\n            photo_dir = os.path.join(base_path, folder, \"photos\")\n            sketch_dir = os.path.join(base_path, folder, \"sketches\")\n            photo_files = sorted(os.listdir(photo_dir))\n            sketch_files = sorted(os.listdir(sketch_dir))\n            for p, s in zip(photo_files, sketch_files):\n                self.photo_paths.append(os.path.join(photo_dir, p))\n                self.sketch_paths.append(os.path.join(sketch_dir, s))\n\n    def __len__(self):\n        return len(self.photo_paths)\n\n    def __getitem__(self, idx):\n        photo = Image.open(self.photo_paths[idx]).convert(\"RGB\")\n        sketch = Image.open(self.sketch_paths[idx]).convert(\"RGB\")\n\n        if self.transform:\n            photo = self.transform(photo)\n            sketch = self.transform(sketch)\n\n        return {\"photo\": photo, \"sketch\": sketch}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:23:04.819745Z","iopub.execute_input":"2025-04-13T09:23:04.820325Z","iopub.status.idle":"2025-04-13T09:23:04.826887Z","shell.execute_reply.started":"2025-04-13T09:23:04.820295Z","shell.execute_reply":"2025-04-13T09:23:04.826194Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\ndataset = FaceSketchDataset(\n    base_path=\"/kaggle/input/person-face-sketches\",\n    folders=[\"train\", \"test\", \"val\"],\n    transform=transform\n)\n\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:23:04.828384Z","iopub.execute_input":"2025-04-13T09:23:04.828639Z","iopub.status.idle":"2025-04-13T09:23:05.589535Z","shell.execute_reply.started":"2025-04-13T09:23:04.828613Z","shell.execute_reply":"2025-04-13T09:23:05.588701Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(channels, channels, 3),\n            nn.InstanceNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(channels, channels, 3),\n            nn.InstanceNorm2d(channels)\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\nclass Generator(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, n_residuals=3):\n        super(Generator, self).__init__()\n        model = [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(in_channels, 64, 7),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True)\n        ]\n\n        curr_dim = 64\n        for _ in range(2):\n            model += [\n                nn.Conv2d(curr_dim, curr_dim * 2, 3, 2, 1),\n                nn.InstanceNorm2d(curr_dim * 2),\n                nn.ReLU(inplace=True)\n            ]\n            curr_dim *= 2\n\n        for _ in range(n_residuals):\n            model += [ResidualBlock(curr_dim)]\n\n        for _ in range(2):\n            model += [\n                nn.ConvTranspose2d(curr_dim, curr_dim // 2, 3, 2, 1, output_padding=1),\n                nn.InstanceNorm2d(curr_dim // 2),\n                nn.ReLU(inplace=True)\n            ]\n            curr_dim //= 2\n\n        model += [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(curr_dim, out_channels, 7),\n            nn.Tanh()\n        ]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:23:05.590473Z","iopub.execute_input":"2025-04-13T09:23:05.590794Z","iopub.status.idle":"2025-04-13T09:23:05.600943Z","shell.execute_reply.started":"2025-04-13T09:23:05.590763Z","shell.execute_reply":"2025-04-13T09:23:05.600238Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_channels=3):\n        super(Discriminator, self).__init__()\n        def block(in_filters, out_filters, normalize=True):\n            layers = [nn.Conv2d(in_filters, out_filters, 4, 2, 1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(in_channels, 64, normalize=False),\n            *block(64, 128),\n            *block(128, 256),\n            *block(256, 512),\n            nn.Conv2d(512, 1, 4, 1, 1)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:23:05.601825Z","iopub.execute_input":"2025-04-13T09:23:05.602159Z","iopub.status.idle":"2025-04-13T09:23:05.622991Z","shell.execute_reply.started":"2025-04-13T09:23:05.602127Z","shell.execute_reply":"2025-04-13T09:23:05.622379Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torchvision.utils import save_image\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch\nimport os\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# =============================\n# Multi-GPU Model Initialization\n# =============================\nG_AB = nn.DataParallel(Generator()).to(device)\nG_BA = nn.DataParallel(Generator()).to(device)\nD_A = nn.DataParallel(Discriminator()).to(device)\nD_B = nn.DataParallel(Discriminator()).to(device)\n\n# =============================\n# Loss Functions & Optimizers\n# =============================\nadversarial_loss = nn.MSELoss()\ncycle_loss = nn.L1Loss()\nidentity_loss = nn.L1Loss()\n\nlr = 0.0002\nbeta1 = 0.5\nbeta2 = 0.999\n\noptimizer_G = torch.optim.Adam(\n    list(G_AB.parameters()) + list(G_BA.parameters()),\n    lr=lr, betas=(beta1, beta2)\n)\noptimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(beta1, beta2))\noptimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(beta1, beta2))\n\n# =============================\n# Folder Setup\n# =============================\nos.makedirs(\"generated_images\", exist_ok=True)\nos.makedirs(\"saved_models\", exist_ok=True)\n\n# =============================\n# Helper to Denormalize Images\n# =============================\ndef denormalize(tensor):\n    return tensor * 0.5 + 0.5\n\n# =============================\n# Resume from Checkpoint Logic\n# =============================\nresume = True  # ğŸ”„ Toggle resume\ncheckpoint_path = \"/kaggle/input/cyclegan-checkpoint/checkpoint.pth\"\nstart_epoch = 1\nnum_epochs = 5  # total epochs you want to train\n\nif resume and os.path.exists(checkpoint_path):\n    print(f\"ğŸ” Resuming training from {checkpoint_path}\")\n    checkpoint = torch.load(checkpoint_path)\n\n    G_AB.load_state_dict(checkpoint['G_AB'])\n    G_BA.load_state_dict(checkpoint['G_BA'])\n    D_A.load_state_dict(checkpoint['D_A'])\n    D_B.load_state_dict(checkpoint['D_B'])\n\n    optimizer_G.load_state_dict(checkpoint['opt_G'])\n    optimizer_D_A.load_state_dict(checkpoint['opt_D_A'])\n    optimizer_D_B.load_state_dict(checkpoint['opt_D_B'])\n\n    start_epoch = checkpoint['epoch'] + 1\nelse:\n    print(\"ğŸ†• Starting training from scratch.\")\n\n# =============================\n# Confirm GPU Usage\n# =============================\nprint(\"ğŸš€ Using GPUs:\")\nprint(\"GPUs used by G_AB:\", G_AB.device_ids)\nprint(\"GPUs used by D_A:\", D_A.device_ids)\n\n# =============================\n# Training Loop\n# =============================\nreal_label = 1.0\nfake_label = 0.0\n\nfor epoch in range(start_epoch, num_epochs + 1):\n    G_losses, D_A_losses, D_B_losses = [], [], []\n    print(f\"ğŸŒŸ Epoch {epoch}/{num_epochs}\")\n    loop = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch}\")\n\n    for i, batch in loop:\n        real_A = batch[\"photo\"].to(device)\n        real_B = batch[\"sketch\"].to(device)\n\n        #### -------- Train Generators -------- ####\n        optimizer_G.zero_grad()\n\n        same_B = G_AB(real_B)\n        loss_identity_B = identity_loss(same_B, real_B) * 5.0\n\n        same_A = G_BA(real_A)\n        loss_identity_A = identity_loss(same_A, real_A) * 5.0\n\n        fake_B = G_AB(real_A)\n        pred_fake_B = D_B(fake_B)\n        loss_GAN_AB = adversarial_loss(pred_fake_B, torch.ones_like(pred_fake_B).to(device))\n\n        fake_A = G_BA(real_B)\n        pred_fake_A = D_A(fake_A)\n        loss_GAN_BA = adversarial_loss(pred_fake_A, torch.ones_like(pred_fake_A).to(device))\n\n        recov_A = G_BA(fake_B)\n        loss_cycle_A = cycle_loss(recov_A, real_A) * 10.0\n\n        recov_B = G_AB(fake_A)\n        loss_cycle_B = cycle_loss(recov_B, real_B) * 10.0\n\n        # Total Generator Loss\n        loss_G = (loss_identity_A + loss_identity_B +\n                  loss_GAN_AB + loss_GAN_BA +\n                  loss_cycle_A + loss_cycle_B)\n        loss_G.backward()\n        optimizer_G.step()\n\n        #### -------- Train Discriminator A -------- ####\n        optimizer_D_A.zero_grad()\n        pred_real_A = D_A(real_A)\n        loss_real_A = adversarial_loss(pred_real_A, torch.ones_like(pred_real_A).to(device))\n\n        pred_fake_A = D_A(fake_A.detach())\n        loss_fake_A = adversarial_loss(pred_fake_A, torch.zeros_like(pred_fake_A).to(device))\n\n        loss_D_A = 0.5 * (loss_real_A + loss_fake_A)\n        loss_D_A.backward()\n        optimizer_D_A.step()\n\n        #### -------- Train Discriminator B -------- ####\n        optimizer_D_B.zero_grad()\n        pred_real_B = D_B(real_B)\n        loss_real_B = adversarial_loss(pred_real_B, torch.ones_like(pred_real_B).to(device))\n\n        pred_fake_B = D_B(fake_B.detach())\n        loss_fake_B = adversarial_loss(pred_fake_B, torch.zeros_like(pred_fake_B).to(device))\n\n        loss_D_B = 0.5 * (loss_real_B + loss_fake_B)\n        loss_D_B.backward()\n        optimizer_D_B.step()\n\n        G_losses.append(loss_G.item())\n        D_A_losses.append(loss_D_A.item())\n        D_B_losses.append(loss_D_B.item())\n\n        loop.set_postfix({\n            \"Loss_G\": f\"{loss_G.item():.4f}\",\n            \"Loss_D_A\": f\"{loss_D_A.item():.4f}\",\n            \"Loss_D_B\": f\"{loss_D_B.item():.4f}\"\n        })\n\n    #### Save Output Images ####\n    save_image(denormalize(fake_B), f\"generated_images/epoch{epoch}_fake_sketch.png\")\n    save_image(denormalize(fake_A), f\"generated_images/epoch{epoch}_fake_photo.png\")\n\n    #### Save Checkpoint ####\n    checkpoint = {\n        'epoch': epoch,\n        'G_AB': G_AB.state_dict(),\n        'G_BA': G_BA.state_dict(),\n        'D_A': D_A.state_dict(),\n        'D_B': D_B.state_dict(),\n        'opt_G': optimizer_G.state_dict(),\n        'opt_D_A': optimizer_D_A.state_dict(),\n        'opt_D_B': optimizer_D_B.state_dict()\n    }\n    torch.save(checkpoint, \"saved_models/checkpoint_latest.pth\")\n\n    print(f\"\\nâœ… Epoch {epoch} completed â¤ \"\n          f\"Generator: {sum(G_losses)/len(G_losses):.4f}, \"\n          f\"D_A: {sum(D_A_losses)/len(D_A_losses):.4f}, \"\n          f\"D_B: {sum(D_B_losses)/len(D_B_losses):.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:23:05.623889Z","iopub.execute_input":"2025-04-13T09:23:05.624175Z","iopub.status.idle":"2025-04-13T11:38:25.378502Z","shell.execute_reply.started":"2025-04-13T09:23:05.624151Z","shell.execute_reply":"2025-04-13T11:38:25.377765Z"}},"outputs":[{"name":"stdout","text":"ğŸ†• Starting training from scratch.\nğŸš€ Using GPUs:\nGPUs used by G_AB: [0, 1]\nGPUs used by D_A: [0, 1]\nğŸŒŸ Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5584/5584 [27:07<00:00,  3.43it/s, Loss_G=3.3378, Loss_D_A=0.1282, Loss_D_B=0.0957]\n","output_type":"stream"},{"name":"stdout","text":"\nâœ… Epoch 1 completed â¤ Generator: 4.1273, D_A: 0.1704, D_B: 0.1393\n\nğŸŒŸ Epoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5584/5584 [27:02<00:00,  3.44it/s, Loss_G=5.3969, Loss_D_A=0.0996, Loss_D_B=0.0402]\n","output_type":"stream"},{"name":"stdout","text":"\nâœ… Epoch 2 completed â¤ Generator: 3.6981, D_A: 0.1435, D_B: 0.0819\n\nğŸŒŸ Epoch 3/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5584/5584 [27:02<00:00,  3.44it/s, Loss_G=3.7132, Loss_D_A=0.0881, Loss_D_B=0.0042]\n","output_type":"stream"},{"name":"stdout","text":"\nâœ… Epoch 3 completed â¤ Generator: 3.5758, D_A: 0.1313, D_B: 0.0590\n\nğŸŒŸ Epoch 4/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5584/5584 [27:01<00:00,  3.44it/s, Loss_G=2.5749, Loss_D_A=0.1358, Loss_D_B=0.1482]\n","output_type":"stream"},{"name":"stdout","text":"\nâœ… Epoch 4 completed â¤ Generator: 3.5170, D_A: 0.1232, D_B: 0.0420\n\nğŸŒŸ Epoch 5/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5584/5584 [27:02<00:00,  3.44it/s, Loss_G=2.9121, Loss_D_A=0.1079, Loss_D_B=0.0287]\n","output_type":"stream"},{"name":"stdout","text":"\nâœ… Epoch 5 completed â¤ Generator: 3.4390, D_A: 0.1180, D_B: 0.0366\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"torch.save(G_AB.module.state_dict(), \"G_AB_trained.pth\")\ntorch.save(G_BA.module.state_dict(), \"G_BA_trained.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:39:46.108306Z","iopub.execute_input":"2025-04-13T11:39:46.108611Z","iopub.status.idle":"2025-04-13T11:39:46.193410Z","shell.execute_reply.started":"2025-04-13T11:39:46.108592Z","shell.execute_reply":"2025-04-13T11:39:46.192459Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom PIL import Image\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load generator safely (weights only)\ngenerator = Generator().to(device)\ngenerator.load_state_dict(\n    torch.load(\"G_AB_trained.pth\", map_location=device, weights_only=True)\n)\ngenerator.eval()\n\n# Same transforms used during training\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\n# Convert image\ndef generate_sketch(input_image_path, output_image_path):\n    image = Image.open(input_image_path).convert(\"RGB\")\n    image = transform(image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        output = generator(image)\n    output = output.squeeze().cpu() * 0.5 + 0.5\n    save_image(output, output_image_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:41:55.134421Z","iopub.execute_input":"2025-04-13T11:41:55.134669Z","iopub.status.idle":"2025-04-13T11:41:55.194597Z","shell.execute_reply.started":"2025-04-13T11:41:55.134653Z","shell.execute_reply":"2025-04-13T11:41:55.194025Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from flask import Flask, request, send_file\nfrom io import BytesIO\n\napp = Flask(__name__)\n\n@app.route(\"/convert\", methods=[\"POST\"])\ndef convert_image():\n    file = request.files['file']\n    img = Image.open(file).convert(\"RGB\")\n    img = transform(img).unsqueeze(0).to(device)\n    with torch.no_grad():\n        output = generator(img)\n    output = output.squeeze().cpu() * 0.5 + 0.5\n\n    buffer = BytesIO()\n    transforms.ToPILImage()(output).save(buffer, format=\"PNG\")\n    buffer.seek(0)\n    return send_file(buffer, mimetype='image/png')\n\nif __name__ == \"__main__\":\n    app.run()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T12:11:52.507582Z","iopub.execute_input":"2025-04-13T12:11:52.508395Z","iopub.status.idle":"2025-04-13T12:39:15.050491Z","shell.execute_reply.started":"2025-04-13T12:11:52.508369Z","shell.execute_reply":"2025-04-13T12:39:15.049812Z"}},"outputs":[{"name":"stdout","text":" * Serving Flask app '__main__'\n * Debug mode: off\n","output_type":"stream"}],"execution_count":16}]}